{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisite:\n",
    "\n",
    "- Understanding of Python\n",
    "\n",
    "- Understanding of Text Analytics and Natural Language Processing \n",
    "\n",
    "**Level of Exercise** : Beginner\n",
    "\n",
    "**Effort in Time** : 120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation of Text Reviews\n",
    "\n",
    "### Objective:\n",
    "  **Musical Instruments data set has review information about musical instruments along with their rating**.\n",
    "   - Here we are extracting the reviewtext from data set and performing data cleaning steps related to Text Reviews\n",
    "   - We are using the NLTK(Natural Language Toolkit)  library for this purpose\n",
    "   - Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, \n",
    "     like speech and text, by software.\n",
    " ### Data Cleaning Tasks\n",
    "      \n",
    "      1.Dropping rows and columns containing missing values.  \n",
    "      2.Removing Duplicate rows depending on the subset of columns chosen.  \n",
    "      3.Sentence Tokenization  \n",
    "      4.Word Tokenization.  \n",
    "      5.Punctuation Removal  \n",
    "      6.Stop Words Removal  \n",
    "      7.Stemming.  \n",
    "      8.Lemmatization   \n",
    "      9.POS Tagging.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the DataSet\n",
    "###  I have considered Musical Instruments dataset from Amazon http://jmcauley.ucsd.edu/data/amazon/\n",
    "The source is  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz  \n",
    "To read the data I have used pandas library   \n",
    "\n",
    "**Tasks Performed in this section  \n",
    "1.Read the csv file  \n",
    "2.Seeing the dimension of the file  \n",
    "3.Seeing the names of varibales in the dataset  \n",
    "4.Seeing the top 10 rows.  \n",
    "5.Seeing the datatypes of the columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Lakshmiholla-2808/first/master/Musical_Instruments_5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See dimension\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display DataTypes of Each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the count of the ratings of each rating category[5,4,3,2,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "     - Identify missing values using isnull() function\n",
    "     - Drop the rows and columns containing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows having missing values\n",
    "df=df.dropna(axis=0, how='any')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle duplicate values.\n",
    "  **Sort the dataframe by  productid(asin) and consider the records having same reviewerId,reviewerName,reviewTime,summary\n",
    "    and reviewText as duplicate rows and drop the rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_f=df.sort_values('asin').drop_duplicates(subset=['reviewerID','reviewerName','reviewTime','summary','reviewText'],keep='first',inplace=False)\n",
    "data_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before cleaning review text cross check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the set of stopwords using stopwords in English module in nltk library\n",
    "  - Stop words are frequently used words which don't add much value to information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizer\n",
    "- Consider the reviewText from the dataframe column review Text.\n",
    "- Use sent_tokenize function to break the review text into a list of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#Function for sentence tokenization\n",
    "def sentence_tokenize(sentence):\n",
    "    return sent_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word Tokenizer\n",
    "- Consider the tokenized sentences  and give it as input for word Tokeniztion\n",
    "- Loop through the sentence list\n",
    "- Tokenize the sentences using inbuilt word_tokenize function in nltk corpus \n",
    "- Save it in a list variable called words.\n",
    "- Remove punctuations\n",
    "- Return the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#Function for word tokenization\n",
    "def myword_tokenize(sentList):\n",
    "    words = list()\n",
    "    for row in sentList:\n",
    "        words = words + word_tokenize(row)\n",
    "        words= list(filter(lambda token: token not in string.punctuation, words))\n",
    "    return words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  StopWords Removal \n",
    "\n",
    "- Consider the tokenized word list  and give it as input for stopwords Removal\n",
    "- Loop through the words and check whether they are in list of stopwords.\n",
    "- If so filter them and return the filtered list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for stop word removal\n",
    "def remove_stopwords(words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stopword_removed_list= [i for i in words if i not in stop_words]\n",
    "    return(stopword_removed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stemming\n",
    "\n",
    "- Stemming: A process of removing and replacing suffixes to get to the root form of the word, which is called stem.\n",
    "- For example, connection, connected, connecting word reduce to a common word \"connect\".\n",
    "- The word list after removing stopwords and punctuations is given as input to the stemming function.\n",
    "- Loop through the words in the word list and apply PorterStemmer.stem() function\n",
    "- The stemmed results are returned in the form of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_words(words):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_words=[]\n",
    "    for w in words:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lemmatization\n",
    "\n",
    "- Lemmatization reduces words to their base word, which is linguistically correct lemmas. \n",
    "- It transforms root word with the use of vocabulary and morphological analysis. \n",
    "-  Lemmatization is usually more sophisticated than stemming. \n",
    "-  Stemmer works on an individual word without knowledge of the context. \n",
    "-  For example, The word \"better\" has \"good\" as its lemma\n",
    "- The word list after removing stopwords and punctuations is given as input to the lemmatized  function.\n",
    "- Loop through the words in the word list and apply WordNetLemmatizer.lemmatize() function\n",
    "- The lemmatized results are returned in the form of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def lemmatize_words(words):\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmatizedwords=[]\n",
    "    for w in words:\n",
    "        lemmatizedwords.append(lem.lemmatize(w,\"v\"))\n",
    "    return lemmatizedwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parts of speech Tagging\n",
    "\n",
    "- The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word.\n",
    "   whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. \n",
    "-  POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "-  The lemmatized list of words are given as input to the POS tagging function\n",
    "-  pos_tag function is used to assign tags such as N(Noun),VB(Verb), NNP(Noun Phrase ) and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for POS tagging\n",
    "def pos_tagging(words):\n",
    "    postagginglist=[]\n",
    "    \n",
    "    postagginglist.append(nltk.pos_tag(words))\n",
    "    return postagginglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Temporary DataFrame to store the results after perform the Data cleaning operation for first 20 records of the dataframe\n",
    " - Columns in the new dataframe are\n",
    " \n",
    "    - **reviewText**: Review Text Information  \n",
    "    - **sent_tokenize**:Review Text tokenized into sentences.  \n",
    "    - **word_tokenized**:Sentence tokenized into words  \n",
    "    - **stop_word_removal**:word list after stop word removal  \n",
    "    - **stemming**:word list after stemming  \n",
    "    - **lemmatize**:word list after lemmatization  \n",
    "    - **postag**:word list assigned with pos tags  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['reviewText'] = df['reviewText'].head(20)\n",
    "#Applying sentence tokenize function\n",
    "temp['sent_tokenized'] = df['reviewText'].head(20).apply(sentence_tokenize)\n",
    "#Applying word tokenize function\n",
    "temp['word_tokenized'] = temp['sent_tokenized'].apply(myword_tokenize)\n",
    "#Applying stopword removal function\n",
    "temp['stop_word_removal'] = temp['word_tokenized'].apply(remove_stopwords)\n",
    "#Applying stemming function\n",
    "temp['stemming'] =temp['stop_word_removal'].apply(stem_words)\n",
    "#Applying lemmatization function\n",
    "temp['lemmatize'] =temp['stop_word_removal'].apply(lemmatize_words)\n",
    "#Applying stop words removal function\n",
    "temp['postag'] =temp['lemmatize'].apply(pos_tagging)\n",
    "\n",
    "temp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the Data Set cell phone accessories in the problem below and write the code for the following questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Lakshmiholla-2808/first/master/cellphone.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Print the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Display the first 10 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Get the count of the ratings of each rating category[5,4,3,2,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Check for missing values and drop rows containing  them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.Create and display a temporary dataframe containing the following columns.\n",
    "  - reviewtext \n",
    "  - tokenized_sentences\n",
    "  - tokenized_words\n",
    "  - stopwords_removed\n",
    "  - stemmed_values\n",
    "  - lemmatized_values\n",
    "  - POStags_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
